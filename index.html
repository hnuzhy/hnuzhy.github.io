<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Huayi Zhou</title>

    <meta name="author" content="Huayi Zhou (周华毅)">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:2.5%;width:63%;vertical-align:middle">
            <p class="name" style="text-align: center;">
              Huayi Zhou (周华毅)
            </p>
            <p>I'm now a Postdoctoral Researcher at <a href="https://www.cuhk.edu.cn/en">The Chinese University of Hong Kong, Shenzhen (CUHK-SZ)</a>. My advisor is <a href="https://sds.cuhk.edu.cn/teacher/1159">Prof. Kui Jia</a>. Before that, I obtained my Master and PhD degrees at <a href="https://www.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a> majoring in <a href="https://www.cs.sjtu.edu.cn/">Computer Science and Engineering</a> in years 2017~2024, and Bachelor degree at <a href="https://www.hnu.edu.cn/">Hunan University (HNU)</a> majoring in Computer Science and Technology in years 2013~2017.
            </p>
            <p>
              My research interests lie in computer vision (e.g., detection, segmentation, and pose estimation), also combining with some machine learning techniques such as <em>multi-task learning</em>, <em>domain adaptation</em>, <em>domain generalization</em> and <em>semi-supervised learning</em>. Lately, I have been exploring vision-language-action based <em>generalizable robot manipulation</em> tasks. I am committed to pursuing <strong>simple yet efficient design</strong>, and designing <strong>data/label efficient learning</strong>. Most of my works are about inferring the physical world (location, association, pose, shape, etc) from RGB images.
            </p>
            <p style="text-align:center">
              <a href="mailto:zhouhuayi@cuhk.edu.cn">Email</a> &nbsp;/&nbsp;
              <a href="https://scholar.google.com/citations?hl=en&user=a876WNUAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
              <a href="https://www.semanticscholar.org/author/Huayi-Zhou/3427881">Semantic Scholar</a> &nbsp;/&nbsp;
              <a href="https://github.com/hnuzhy/">Github</a> &nbsp;/&nbsp;
              <a href="https://www.zhihu.com/people/zhying-34">Zhihu</a>
            </p>
          </td>
          <td style="padding:2.5%;width:40%;max-width:40%">
            <a href="images/HuayiZhou.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/HuayiZhou.jpg" class="hoverZoomLink"></a>
          </td>
        </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>News</h2>
            <p>
              ● [2025.04] Our work <a href="https://hnuzhy.github.io/projects/YOTO/">YOTO</a> for Bimanual Robotic Manipulation is accepted by <strong>Robotics: Science and Systems (RSS) 2025</strong>.
              <br>
              ● [2025.01] Our <a href="https://hnuzhy.github.io/projects/YOTO/">YOTO</a> for <strong>Learning One-Shot Bimanual Robotic Manipulation from Video Demonstrations</strong> is released.
              <br>
              ● [2024.07] I have joined the <a href="https://sds.cuhk.edu.cn/en">School of Data Science</a> at CUHK-SZ being a postdoctoral researcher.
              <br>
              ● [2024.05] I have passed the doctoral dissertation defense on <strong>May 29, 2024</strong>.
              <br>
              ● [2024.04] Our <a href="https://hnuzhy.github.io/projects/SemiUHPE/">SemiUHPE</a> for <strong>Semi-Supervised Unconstrained Head Pose Estimation in the Wild</strong> is released.
              <br>
              ● [2024.02] Our <a href="https://arxiv.org/abs/2402.11566">MultiAugs</a> for boosting <strong>Semi-Supervised 2D Human Pose Estimation</strong> is released.
              <br>
              ● [2024.01] <a href="https://openreview.net/forum?id=pPh9p8anUi">PBADet: A One-Stage Anchor-Free Approach for Part-Body Association</a> is accepted by <strong>ICLR 2024</strong>.
              <br>
              ● [2024.01] <a href="https://hnuzhy.github.io/projects/BPJDet">BPJDet: Extended Object Representation for Generic Body-Part Joint Detection</a> is accepted by <strong>TPAMI 2024</strong>.
              <br>
              ● [2023.07] I've received the <a href="/data/ICME2023-BestStudentPaperRunnerUp.pdf">ICME 2023 Best Student Paper Runner Up Award</a> in <strong>14 July 2023, Brisbane, Australia</strong>.
              <br>
              ● [2023.04] <strong>CONFETI</strong> for Domain Adaptive Semantic Segmentation has been accepted by <strong>CVPR Workshop 2023</strong>.
              <br>
              ● [2023.04] <strong>BPJDet</strong> for Human Body-Part Joint Detection and Association has been accepted by <strong>ICME 2023</strong>.
              <br>
              ● [2023.03] <strong>StuArt</strong> for Individualized Classroom Observation of Students has been accepted by <strong>ICASSP 2023</strong>. <em>(Note: StuArt is one of the key part of the project <a href="https://hnuzhy.github.io/projects/AIClass">AIClass: Automatic Teaching Assistance System Towards Classrooms for K-12 Education</a>.)</em>
              <br>
              ● [2023.03] <strong>SSDA-YOLO</strong> for YOLOv5-based Domain Adaptive Object Detection has been accepted by <strong>CVIU 2023</strong>.
              <br>
              ● [2023.02] Our work <a href="https://hnuzhy.github.io/projects/DirectMHP/">DirectMHP: Direct 2D Multi-Person Head Pose Estimation with Full-range Angles</a> is released.
            </p>
          </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Activities</h2>
            <p>
              ● <strong>Conference Reviewer:</strong> ACMMM'2025, ICCV'2025, ICME'2025, CVPR'2025, ICLR'2025, AAAI'2025, ACCV'2024, ECCV'2024, ICME'2024, CVPR'2024, ICCV'2023, CVPR'2023, ECCV'2022
              <br>
              ● <strong>Journal Reviewer:</strong> Transactions on Image Processing (TIP), Transactions on Circuits and Systems for Video Technology (TCSVT), Transactions on Intelligent Vehicles (TIV)
            </p>
          </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:2.5%;width:100%;max-width:100%">
            <h2>Research Summary</h2>
            <a href="images/Research_Summary_V1.png"><img style="width:100%;max-width:100%;" alt="Research_Summary_V1" src="images/Research_Summary_V1.png"></a>
          </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <h2>Publications</h2>
          </td>
        </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>




        <tr bgcolor="#ffffee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2025_YOTO.png" alt="YOTO" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2501.14208" id="SemiUHPE">
              <span class="papertitle">You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>,  <a href="https://openreview.net/profile?id=~Ruixiang_Wang3">Ruixiang Wang</a>, <a href="https://openreview.net/profile?id=~Yunxin_Tai2">Yunxin Tai</a>, <a href="https://github.com/yuecideng">Yueci Deng</a>, <a href="http://guiliang.me/">Guiliang Liu</a>, <a href="http://kuijia.site/">Kui Jia</a>
            <br>
            <em>Robotics: Science and Systems (RSS)</em>, 2025
            <br>
            <a href="https://hnuzhy.github.io/projects/YOTO">Project</a> / <a href="https://arxiv.org/abs/2501.14208">arXiv</a> / <a href="https://github.com/hnuzhy/YOTO">Github</a>
            <p><strong>TLDR:</strong> This work proposes the YOTO (You Only Teach Once), which can extract and then inject patterns of bimanual actions from as few as a single binocular observation of hand movements, and teach dual robot arms various complex tasks.</p>
          </td>
        </tr>

        <tr bgcolor="#ffe5ee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2024_SemiUHPE.png" alt="SemiUHPE" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2404.02544" id="SemiUHPE">
              <span class="papertitle">Semi-Supervised Unconstrained Head Pose Estimation in the Wild</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>,  <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ">Fei Jiang</a>, <a href="https://scholar.google.com/citations?user=S1JGPCMAAAAJ&hl=en">Jin Yuan</a>, <a href="https://scholar.google.com/citations?user=rCGsLtcAAAAJ&hl=en">Yong Rui</a>, <a href="https://scholar.google.com.tw/citations?user=GtNuBJcAAAAJ">Hongtao Lu</a>, <a href="http://kuijia.site/">Kui Jia</a>
            <br>
            <em>arXiv</em>, 2024
            <br>
            <a href="https://hnuzhy.github.io/projects/SemiUHPE">Project</a> / <a href="https://arxiv.org/abs/2404.02544">arXiv</a> / <a href="https://github.com/hnuzhy/SemiUHPE">Github</a>
            <p><strong>TLDR:</strong> We propose the first semi-supervised unconstrained head pose estimation (SemiUHPE) method, which can leverage a large amount of unlabeled wild head images. SemiUHPE is robust to estimate wild challenging heads (e.g., heavy blur, extreme illumination, severe occlusion, atypical pose, and invisible face).</p>
          </td>
        </tr>
          
        <tr bgcolor="#ffffee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2024_MultiAugs.png" alt="MultiAugs" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2402.11566" id="MultiAugs">
              <span class="papertitle">Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data Augmentation and Consistency Training</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>,  Mukun Luo, <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ">Fei Jiang</a>, <a href="https://scholar.google.com/citations?user=wwNa3wMAAAAJ">Yue Ding</a>, <a href="https://scholar.google.com.tw/citations?user=GtNuBJcAAAAJ">Hongtao Lu</a>
            <br>
            <em>arXiv</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2402.11566">arXiv</a> / <a href="https://github.com/hnuzhy/MultiAugs">Github</a>
            <p><strong>TLDR:</strong> Our method MultiAugs contains two vital components: (1) New advanced collaborative augmentation combinations; (2) Multi-path predictions of strongly augment inputs with diverse augmentations. Either of them can help to boost the performance of Semi-Supervised 2D Human Pose Estimation.</p>
          </td>
        </tr>
          
        <tr bgcolor="#ffe5ee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2024_PBADet.jpg" alt="PBADet" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://openreview.net/forum?id=pPh9p8anUi" id="PBADet">
              <span class="papertitle">PBADet: A One-Stage Anchor-Free Approach for Part-Body Association</span>
            </a>
            <br>
            <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <strong>Huayi Zhou</strong>, <a href="https://scholar.google.com/citations?user=zeGKyiEAAAAJ">Abhishek Sharma</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ">Terrence Chen</a>, <a href="http://wuziyan.com/">Ziyan Wu</a>
            <br>
            <em>International Conference on Learning Representations (ICLR)</em>, 2024 Poster
            <br>
            <a href="https://openreview.net/forum?id=pPh9p8anUi">Paper</a> / <a href="https://arxiv.org/abs/2402.07814">arXiv</a>
            <p><strong>TLDR:</strong> This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection. Building upon the anchor-free object representation across multi-scale feature maps, it introduces a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies.</p>
          </td>
        </tr>
          
        <tr bgcolor="#ffffee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_BPJDetPlus.jpg" alt="BPJDetPlus" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://hnuzhy.github.io/projects/BPJDet" id="BPJDetPlus">
              <span class="papertitle">BPJDet: Extended Object Representation for Generic Body-Part Joint Detection</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ">Fei Jiang</a>, Jiaxin Si, <a href="https://scholar.google.com/citations?user=wwNa3wMAAAAJ">Yue Ding</a>, <a href="https://scholar.google.com.tw/citations?user=GtNuBJcAAAAJ">Hongtao Lu</a>
            <br>
            <em>Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2024
            <br>
            <a href="https://hnuzhy.github.io/projects/BPJDet">Project</a> / <a href="https://ieeexplore.ieee.org/document/10400895">Paper</a> / <a href="https://arxiv.org/abs/2304.10765">arXiv</a> / <a href="https://github.com/hnuzhy/BPJDet/tree/BPJDetPlus">Github</a>
            <p><strong>TLDR:</strong> The journal version of BPJDet. It has various new functions (Multiple Body-Parts Joint Detection and two downstream applications including Body-Head for Accurate Crowd Counting and Body-Hand for Hand Contact Estimation)</p>
          </td>
        </tr>
        
        <tr bgcolor="#ffe5ee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_CONFETI.jpg" alt="CONFETI" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://github.com/cxa9264/CONFETI" id="CONFETI">
              <span class="papertitle">Contrast, Stylize and Adapt: Unsupervised Contrastive Learning Framework for Domain Adaptive Semantic Segmentation</span>
            </a>
            <br>
            Tianyu Li, <a href="https://roysubhankar.github.io/">Subhankar Roy</a>, <strong>Huayi Zhou</strong>, <a href="https://scholar.google.com.tw/citations?user=GtNuBJcAAAAJ">Hongtao Lu</a>, <a href="https://stelat.eu/">Stephane Lathuiliere</a>
            <br>
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, 2023
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Li_Contrast_Stylize_and_Adapt_Unsupervised_Contrastive_Learning_Framework_for_Domain_CVPRW_2023_paper.html">Paper</a> / <a href="https://arxiv.org/abs/2306.09098">arXiv</a> / <a href="https://github.com/cxa9264/CONFETI">Github</a>
            <p><strong>TLDR:</strong> To overcome the domain gap between synthetic and real-world datasets for semantic segmentation, this paper present CONtrastive FEaTure and pIxel alignment (CONFETI) for bridging the domain gap at both the pixel and feature levels.</p>
          </td>
        </tr>

        <tr bgcolor="#ffffee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_BPJDet.png" alt="BPJDet" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://hnuzhy.github.io/projects/BPJDet" id="BPJDet">
              <span class="papertitle">Body-Part Joint Detection and Association via Extended Object Representation</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ">Fei Jiang</a>, <a href="https://scholar.google.com.tw/citations?user=GtNuBJcAAAAJ">Hongtao Lu</a>
            <br>
            <em>IEEE International Conference on Multimedia and Expo (ICME)</em>, 2023 Oral, <font color="red"><strong>(Best Student Paper Runner Up Award)</strong></font>
            <br>
            <a href="https://hnuzhy.github.io/projects/BPJDet">Project</a> / <a href="https://ieeexplore.ieee.org/abstract/document/10219738/">Paper</a> / <a href="https://arxiv.org/abs/2212.07652">arXiv</a> / <a href="https://github.com/hnuzhy/BPJDet">Github</a> / <a href="https://news.sjtu.edu.cn/jdyw/20230809/186979.html">News</a>
            <p><strong>TLDR:</strong> A novel extended object representation that integrates the center location offsets of body or its parts, and construct a dense one-stage Body-Part Joint Detector (BPJDet). This design is simple yet efficient.</p>
          </td>
        </tr>

        <tr bgcolor="#ffe5ee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_StuArt.jpg" alt="StuArt" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://github.com/hnuzhy/StuArt" id="StuArt">
              <span class="papertitle">StuArt: Individualized Classroom Observation of Students with Automatic Behavior Recognition And Tracking</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ">Fei Jiang</a>, Jiaxin Si, Lili Xiong, <a href="https://scholar.google.com.tw/citations?user=GtNuBJcAAAAJ">Hongtao Lu</a>
            <br>
            <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023 Oral
            <br>
            <a href="https://hnuzhy.github.io/projects/AIClass">Project</a> / <a href="https://ieeexplore.ieee.org/abstract/document/10094982">Paper</a> / <a href="https://arxiv.org/abs/2211.03127">arXiv</a> / <a href="https://github.com/hnuzhy/StuArt">Github</a>
            <p><strong>TLDR:</strong> StuArt is a novel automatic system designed for the individualized classroom observation. We proposed some pedagogical approaches in signal processing for K-12 education. <em>(Note: StuArt is one of the key part of the project AIClass.)</em></p>
          </td>
        </tr>

        <tr bgcolor="#ffffee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_DirectMHP.png" alt="DirectMHP" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://hnuzhy.github.io/projects/DirectMHP" id="DirectMHP">
              <span class="papertitle">DirectMHP: Direct 2D Multi-Person Head Pose Estimation with Full-range Angles</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ">Fei Jiang</a>, <a href="https://scholar.google.com.tw/citations?user=GtNuBJcAAAAJ">Hongtao Lu</a>
            <br>
            <em>arXiv</em>, 2023
            <br>
            <a href="https://hnuzhy.github.io/projects/DirectMHP">Project</a> / <a href="https://arxiv.org/abs/2302.01110">arXiv</a> / <a href="https://github.com/hnuzhy/DirectMHP">Github</a>
            <p><strong>TLDR:</strong> This paper focuses on the full-range Multi-Person Head Pose Estimation (MPHPE) problem. We firstly construct two benchmarks by extracting GT labels for head detection and head orientation from public datasets AGORA and CMU Panoptic. Then, we propose a direct end-to-end simple baseline named DirectMHP based on YOLOv5.</p>
          </td>
        </tr>
          
        <tr bgcolor="#ffe5ee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_SSDA-YOLO.jpg" alt="SSDA-YOLO" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://github.com/hnuzhy/SSDA-YOLO" id="SSDA-YOLO">
              <span class="papertitle">SSDA-YOLO: Semi-supervised Domain Adaptive YOLO for Cross-Domain Object Detection</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ">Fei Jiang</a>, <a href="https://scholar.google.com.tw/citations?user=GtNuBJcAAAAJ">Hongtao Lu</a>
            <br>
            <em>Computer Vision and Image Understanding (CVIU)</em>, 2023
            <br>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314223000292">Paper</a> / <a href="https://arxiv.org/abs/2211.02213">arXiv</a> / <a href="https://github.com/hnuzhy/SSDA-YOLO">Github</a>
            <p><strong>TLDR:</strong> This paper presents a novel semi-supervised domain adaptive YOLO (SSDA-YOLO) based method to improve cross-domain detection performance by integrating the compact one-stage stronger detector YOLOv5 with domain adaptation.</p>
          </td>
        </tr>

        <tr bgcolor="#ffffee">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2018_HandMatchingLocation.png" alt="HandMatching" width="240" height="180" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://proceedings.mlr.press/v95/zhou18a.html" id="HandMatching">
              <span class="papertitle">Who Are Raising Their Hands? Hand-Raiser Seeking Based on Object Detection and Pose Estimation</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ">Fei Jiang</a>, <a href="https://scholar.google.com/citations?user=yH74l_8AAAAJ">Ruimin Shen</a>
            <br>
            <em>Asian Conference on Machine Learning (ACML)</em>, 2018 Oral
            <br>
            <a href="https://proceedings.mlr.press/v95/zhou18a.html">Paper</a>
            <p><strong>TLDR:</strong> An automatic hand-raiser recognition algorithm to show who raise their hands in real classroom scenarios, which is of great importance for further analyzing the learning states of individuals.</p>
          </td>
        </tr>


        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <h2>Experience</h2>
          </td>
        </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

        <tr>
          <td style="padding:20px;width:12%;vertical-align:middle"><img src="images/logo_CUHKSZ.png" height="75"></td>
          <td width="75%" valign="center">
            <strong>The Chinese University of Hong Kong, Shenzhen</strong>,
            <br>
            Postdoctoral researcher in School of Data Science
            <br>
            Advisor: <a href="https://sds.cuhk.edu.cn/teacher/1159">Prof. Kui Jia</a>
            <br>
            2024.7 - Present
          </td>
        </tr>
          
        <tr>
          <td style="padding:20px;width:12%;vertical-align:middle"><img src="images/logo_SJTU.png" height="75"></td>
          <td width="75%" valign="center">
            <strong>Shanghai Jiao Tong University</strong>,
            <br>
            Ph.D. student in Computer Science and Engineering
            <br>
            Advisor: <a href="https://www.cs.sjtu.edu.cn/PeopleDetail.aspx?id=95">Prof. Hongtao Lu</a>
            <br>
            2020.9 - 2024.06
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:12%;vertical-align:middle"><img src="images/logo_snapdragon.png" height="75"></td>
          <td width="75%" valign="center">
            <strong>Qualcomm Wireless Communication Technologies (Shenzhen, China)</strong>,
            <br>
            Engineering Intern at AI Department, Machine Learning Group (MLGCN)
            <br>
            Reporting to <a href="https://www.linkedin.com/in/zhou-dongyong-94a83775/">Dongyong Zhou, Senior Software Engineer</a>
            <br>
            2019.6 - 2019.10
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:12%;vertical-align:middle"><img src="images/logo_SJTU.png" height="75"></td>
          <td width="75%" valign="center">
            <strong>Shanghai Jiao Tong University</strong>,
            <br>
            Academic Master student in Computer Science and Engineering
            <br>
            Advisor: <a href="https://scholar.google.com/citations?hl=en&user=yH74l_8AAAAJ">Prof. Ruimin Shen</a>
            <br>
            2017.9 - 2020.3
          </td>
        </tr>
        
        <tr>
          <td style="padding:20px;width:12%;vertical-align:middle"><img src="images/logo_HNU.png" height="75"></td>
          <td width="75%" valign="center">
            <strong>Hunan University</strong>,
            <br>
            Bachelor of Engineering in Computer Science and Technology
            <br>
            2013.9 - 2017.6
          </td>
        </tr>
          
        </tbody></table>

        <table width="40%" align="center" border="0" cellpadding="20"><tbody>
        <!--
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=miHMag0r0bgkJUksRL-UvmujX7WAORtBiJCf-dSUAlM&cl=ffffff&w=a"></script>
        <script type="text/javascript" id="mapmyvisitors" src="https://mapmyvisitors.com/map.js?cl=bbbbbb&w=a&t=tt&d=3IvWcMNchklQveOAW_RnSMVtGQYI3pSONsOuMyDQUx0&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>
        -->
        <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=3IvWcMNchklQveOAW_RnSMVtGQYI3pSONsOuMyDQUx0&cl=ffffff&w=a"></script>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
              The template of this website is borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jonathan T. Barron</a>.
            </p>
          </td>
        </tr>
        </tbody></table>
          
        </td>
      </tr>
    </table>
  </body>
</html>
