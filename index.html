<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Huayi Zhou</title>

    <meta name="author" content="Huayi Zhou (周华毅)">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:2.5%;width:63%;vertical-align:middle">
            <p class="name" style="text-align: center;">
              Huayi Zhou (周华毅)
            </p>
            <p>I'm a PhD student at <a href="https://www.cs.sjtu.edu.cn/">Department of Computer Science and Engineering</a>, Shanghai Jiao Tong University (SJTU), advised by <a href="https://www.cs.sjtu.edu.cn/PeopleDetail.aspx?id=95">Prof. Hongtao Lu</a> from year 2020. Previously, I was a master student at SJTU advised by <a href="https://scholar.google.com/citations?hl=en&user=yH74l_8AAAAJ">Prof. Ruimin Shen</a> from year 2017 to 2020, and an undergraduate student at Hunan University (HNU) majoring in Computer Science and Technology from year 2013 to 2017.
            </p>
            <p>
              My research interests lie in computer vision (e.g., object detection and pose estimation), also combining with some machine learning techniques such as multi-task learning, domain adaptation and semi-supervised learning. I am committed to pursuing <strong>simple yet efficient design</strong>, and explore <strong>data/label efficient learning</strong>. Most of my works is about inferring the physical world (location, association, pose, shape, etc) from RGB images. Representative papers are <span class="highlight">highlighted</span>.
            </p>
            <p style="text-align:center">
              <a href="mailto:sjtu_zhy@sjtu.edu.cn">Email</a> &nbsp;/&nbsp;
              <a href="https://scholar.google.com/citations?hl=en&user=a876WNUAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
              <a href="https://www.linkedin.com/in/%E5%8D%8E%E6%AF%85-%E5%91%A8-87264a14b/">LinkedIn</a> &nbsp;/&nbsp;
              <a href="https://github.com/hnuzhy/">Github</a> &nbsp;/&nbsp;
              <a href="https://www.zhihu.com/people/zhying-34">Zhihu</a>
            </p>
          </td>
          <td style="padding:2.5%;width:40%;max-width:40%">
            <a href="images/HuayiZhou.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/HuayiZhou.jpg" class="hoverZoomLink"></a>
          </td>
        </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>News</h2>
            <p>
              ● [2023.07] I've received the <a href="/data/ICME2023-BestStudentPaperRunnerUp.pdf">ICME 2023 Best Student Paper Runner Up Award</a>
              <br>
              ● [2023.04] <strong>BPJDet</strong> for Human Body-Part Joint Detection and Association has been accepted by ICME2023.
              <br>
              ● [2023.03] <strong>StuArt</strong> for Individualized Classroom Observation of Students has been accepted by ICASSP2023.
            </p>
          </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Activities</h2>
            <p>
              ● <strong>Conference Reviewer:</strong> CVPR'2024, ICCV'2023, CVPR'2023, ECCV'2022
              <br>
              ● <strong>Journal Reviewer:</strong> Transactions on Image Processing (TIP), Transactions on Circuits and Systems for Video Technology (TCSVT), Transactions on Intelligent Vehicles (TIV)
            </p>
          </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <h2>Publications</h2>
          </td>
        </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

        <tr bgcolor="#ffffd0">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_BPJDetPlus.jpg" alt="BPJDetPlus" width="180" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://hnuzhy.github.io/projects/BPJDet" id="BPJDetPlus">
              <span class="papertitle">BPJDet: Extended Object Representation for Generic Body-Part Joint Detection</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, Fei Jiang, Jiaxin Si, Yue Ding, Hongtao Lu
            <br>
            <em>Arxiv</em>, 2023 (submitted to TPAMI)
            <br>
            <a href="https://hnuzhy.github.io/projects/BPJDet">Project</a> / <a href="https://arxiv.org/abs/2304.10765">Arxiv</a> / <a href="https://github.com/hnuzhy/BPJDet/tree/BPJDetPlus">Github</a>
            <p>The journal version of BPJDet. It has various new functions (Multiple Body-Parts Joint Detection and two downstream applications including Body-Head for Accurate Crowd Counting and Body-Hand for Hand Contact Estimation)</p>
          </td>
        </tr>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_CONFETI.jpg" alt="CONFETI" width="180" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://github.com/cxa9264/CONFETI" id="CONFETI">
              <span class="papertitle">Contrast, Stylize and Adapt: Unsupervised Contrastive Learning Framework for Domain Adaptive Semantic Segmentation</span>
            </a>
            <br>
            Tianyu Li, Subhankar Roy, <strong>Huayi Zhou</strong>, Hongtao Lu, Stephane Lathuiliere
            <br>
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, 2023
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Li_Contrast_Stylize_and_Adapt_Unsupervised_Contrastive_Learning_Framework_for_Domain_CVPRW_2023_paper.html">Paper</a> / <a href="https://arxiv.org/abs/2306.09098">Arxiv</a> / <a href="https://github.com/cxa9264/CONFETI">Github</a>
            <p>To overcome the domain gap between synthetic and real-world datasets for semantic segmentation, this paper present CONtrastive FEaTure and pIxel alignment (CONFETI) for bridging the domain gap at both the pixel and feature levels.</p>
          </td>
        </tr>

        <tr bgcolor="#ffffd0">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_BPJDet.png" alt="BPJDet" width="180" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://hnuzhy.github.io/projects/BPJDet" id="BPJDet">
              <span class="papertitle">Body-Part Joint Detection and Association via Extended Object Representation</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, Fei Jiang, Hongtao Lu
            <br>
            <em>IEEE International Conference on Multimedia and Expo (ICME)</em>, 2023 Oral, <font color="red"><strong>(Best Student Paper Runner Up Award)</strong></font>
            <br>
            <a href="https://hnuzhy.github.io/projects/BPJDet">Project</a> / <a href="https://ieeexplore.ieee.org/abstract/document/10219738/">Paper</a> / <a href="https://arxiv.org/abs/2212.07652">Arxiv</a> / <a href="https://github.com/hnuzhy/BPJDet">Github</a> / <a href="https://news.sjtu.edu.cn/jdyw/20230809/186979.html">News</a>
            <p>A novel extended object representation that integrates the center location offsets of body or its parts, and construct a dense one-stage Body-Part Joint Detector (BPJDet). This design is simple yet efficient.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_StuArt.jpg" alt="StuArt" width="180" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://github.com/hnuzhy/StuArt" id="StuArt">
              <span class="papertitle">StuArt: Individualized Classroom Observation of Students with Automatic Behavior Recognition And Tracking</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, Fei Jiang, Jiaxin Si, Lili Xiong, Hongtao Lu
            <br>
            <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023 Oral
            <br>
            <a href="https://ieeexplore.ieee.org/abstract/document/10094982">Paper</a> / <a href="https://arxiv.org/abs/2211.03127">Arxiv</a> / <a href="https://github.com/hnuzhy/StuArt">Github</a>
            <p>StuArt is a novel automatic system designed for the individualized classroom observation. We proposed some pedagogical approaches in signal processing for K-12 education.</p>
          </td>
        </tr>

        <tr bgcolor="#ffffd0">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_DirectMHP.png" alt="DirectMHP" width="180" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://github.com/hnuzhy/DirectMHP" id="DirectMHP">
              <span class="papertitle">DirectMHP: Direct 2D Multi-Person Head Pose Estimation with Full-range Angles</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, Fei Jiang, Hongtao Lu
            <br>
            <em>Arxiv</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2302.01110">Arxiv</a> / <a href="https://github.com/hnuzhy/DirectMHP">Github</a>
            <p>This paper focuses on the full-range Multi-Person Head Pose Estimation (MPHPE) problem. We firstly construct two benchmarks by extracting GT labels for head detection and head orientation from public datasets AGORA and CMU Panoptic. Then, we propose a direct end-to-end simple baseline named DirectMHP based on YOLOv5.</p>
          </td>
        </tr>
          
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023_SSDA-YOLO.jpg" alt="SSDA-YOLO" width="180" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://github.com/hnuzhy/SSDA-YOLO" id="SSDA-YOLO">
              <span class="papertitle">SSDA-YOLO: Semi-supervised Domain Adaptive YOLO for Cross-Domain Object Detection</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, Fei Jiang, Hongtao Lu
            <br>
            <em>Computer Vision and Image Understanding (CVIU)</em>, 2023
            <br>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314223000292">Paper</a> / <a href="https://arxiv.org/abs/2211.02213">Arxiv</a> / <a href="https://github.com/hnuzhy/SSDA-YOLO">Github</a>
            <p>This paper presents a novel semi-supervised domain adaptive YOLO (SSDA-YOLO) based method to improve cross-domain detection performance by integrating the compact one-stage stronger detector YOLOv5 with domain adaptation.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2018_HandMatching.jpg" alt="HandMatching" width="180" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://proceedings.mlr.press/v95/zhou18a.html" id="HandMatching">
              <span class="papertitle">Who Are Raising Their Hands? Hand-Raiser Seeking Based on Object Detection and Pose Estimation</span>
            </a>
            <br>
            <strong>Huayi Zhou</strong>, Fei Jiang, Ruimin Shen
            <br>
            <em>Asian Conference on Machine Learning (ACML)</em>, 2018 Oral
            <br>
            <a href="https://proceedings.mlr.press/v95/zhou18a.html">Paper</a>
            <p>An automatic hand-raiser recognition algorithm to show who raise their hands in real classroom scenarios.</p>
          </td>
        </tr>


        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <h2>Experience</h2>
          </td>
        </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        
        <tr>
          <td style="padding:20px;width:12%;vertical-align:middle"><img src="images/logo_SJTU.png" height="100"></td>
          <td width="75%" valign="center">
            <strong>Shanghai Jiao Tong University</strong>,
            <br>
            Ph.D. student in Computer Science and Engineering
            <br>
            Advisor: <a href="https://www.cs.sjtu.edu.cn/PeopleDetail.aspx?id=95">Prof. Hongtao Lu</a>
            <br>
            2020.9 - Present
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:12%;vertical-align:middle"><img src="images/logo_snapdragon.png" height="100"></td>
          <td width="75%" valign="center">
            <strong>Qualcomm Wireless Communication Technologies (Shenzhen, China)</strong>,
            <br>
            Engineering Intern at AI Department, Machine Learning Group (MLGCN)
            <br>
            Reporting to <a href="https://www.linkedin.com/in/zhou-dongyong-94a83775/">Dongyong Zhou, Senior Software Engineer</a>
            <br>
            2019.6 - 2019.10
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:12%;vertical-align:middle"><img src="images/logo_SJTU.png" height="100"></td>
          <td width="75%" valign="center">
            <strong>Shanghai Jiao Tong University</strong>,
            <br>
            Academic Master student in Computer Science and Engineering
            <br>
            Advisor: <a href="https://scholar.google.com/citations?hl=en&user=yH74l_8AAAAJ">Prof. Ruimin Shen</a>
            <br>
            2017.9 - 2020.3
          </td>
        </tr>
        
        <tr>
          <td style="padding:20px;width:12%;vertical-align:middle"><img src="images/logo_HNU.png" height="100"></td>
          <td width="75%" valign="center">
            <strong>Hunan University</strong>,
            <br>
            Bachelor of Engineering in Computer Science and Technology
            <br>
            2013.9 - 2017.6
          </td>
        </tr>
          
        </tbody></table>

        <table width="75%" align="center" border="0" cellpadding="20"><tbody>
        <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=3IvWcMNchklQveOAW_RnSMVtGQYI3pSONsOuMyDQUx0&cl=ffffff&w=a"></script>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
              The template of this website is borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jonathan T. Barron</a>.
            </p>
          </td>
        </tr>
        </tbody></table>
          
        </td>
      </tr>
    </table>
  </body>
</html>
