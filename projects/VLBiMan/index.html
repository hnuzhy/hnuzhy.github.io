
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>VLBiMan</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://hnuzhy.github.io/projects/BPJDet/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://hnuzhy.github.io/projects/VLBiMan/"/>
    <meta property="og:title" content="Vision-Language Anchored One-Shot Demonstration Enables Generalizable Bimanual Manipulation" />
    <meta property="og:description" content="Achieving generalizable bimanual manipulation requires systems that learn efficiently from minimal human input while adapting to real-world uncertainties. Current approaches face a dilemma: imitation policy learning demands exhaustive demonstrations to handle task variations, while modular methods lack flexibility in dynamic environments. We propose a framework that extracts reusable skills from a single human example through methodical decomposition, which preserves invariant primitives as task anchors while dynamically adapts adjustable components via vision-language grounding. This adaptation mechanism resolves scene ambiguities caused by background changing, object repositioning, or visual clutters without policy retraining, leveraging semantic scene parsing and geometric feasibility constraints. Crucially, the system inherits human-like hybrid control capabilities, enabling mixed drive of both arms synchronously and asynchronously. Experiments validate the framework across diverse tasks, including tool-use and multi-object manipulation, demonstrating three key advances: (1) significant reduction in demonstration requirements compared to imitation baselines, (2) compositional generalization through atomic skill splicing for long-horizon tasks, and (3) robustness to new similar objects and external dynamic interference. By bridging human demonstration priors with vision-language grounded adaptation, our work enables dual-arm systems to autonomously adapt to environmental changes while preserving spatiotemporal coordination, which is a critical step toward practical deployment in unstructured settings. "/>



<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🤖</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-10 col-md-offset-1 text-center">
                Vision-Language Anchored One-Shot Demonstration Enables Generalizable Bimanual Manipulation</br> 
            </h2>
        </div>
        <div class="row">
            <div class="col-md-10 col-md-offset-1 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://hnuzhy.github.io">Huayi Zhou</a>
                        </br>The Chinese University of Hong Kong, Shenzhen
                    </li>
                    <li>
                        <a href="http://kuijia.site">Kui Jia</a>
                        </br>The Chinese University of Hong Kong, Shenzhen; DexForce, Shenzhen
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="manu_VLBiMan.pdf">
                            <image src="img/logo_document.png" height="50px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
						<li>
                            <a href="https://arxiv.org/abs/2507.xxxxx">
                            <image src="img/logo_arXiv.png" height="50px">
                                <h4><strong>arXiv</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/hnuzhy/">
                            <image src="img/logo_github.png" height="50px">
                                <h4><strong>Code (coming soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
					Achieving generalizable bimanual manipulation requires systems that learn efficiently from minimal human input while adapting to real-world uncertainties. Current approaches face a dilemma: imitation policy learning demands exhaustive demonstrations to handle task variations, while modular methods lack flexibility in dynamic environments. We propose a framework that extracts reusable skills from a single human example through methodical decomposition, which preserves invariant primitives as task anchors while dynamically adapts adjustable components via vision-language grounding. This adaptation mechanism resolves scene ambiguities caused by background changing, object repositioning, or visual clutters without policy retraining, leveraging semantic scene parsing and geometric feasibility constraints. Crucially, the system inherits human-like hybrid control capabilities, enabling mixed drive of both arms synchronously and asynchronously. Experiments validate the framework across diverse tasks, including tool-use and multi-object manipulation, demonstrating three key advances: (1) significant reduction in demonstration requirements compared to imitation baselines, (2) compositional generalization through atomic skill splicing for long-horizon tasks, and (3) robustness to new similar objects and external dynamic interference. By bridging human demonstration priors with vision-language grounded adaptation, our work enables dual-arm systems to autonomously adapt to environmental changes while preserving spatiotemporal coordination, which is a critical step toward practical deployment in unstructured settings.
                </p>
            </div>
        </div>



        <div class="row">
    		<div class="col-md-10 col-md-offset-1">
				<h3>
                    ▶ Overview and Framework of VLBiMan
                </h3>
                <p class="text-justify">
					Our contributions in this research are threefold: <strong>(1)</strong> We propose VLBiMan, a novel framework that enables generalizable bimanual manipulation through one-shot demonstration and vision-language anchoring, without retraining. <strong>(2)</strong> We introduce a task-aware motion decomposition and adaptation mechanism, allowing for effective reuse of invariant sub-skills via object-centric anchors derived from VLMs. <strong>(3)</strong> We demonstrate the effectiveness of VLBiMan across ten diverse bimanual manipulation tasks, significantly outperforming strong baselines in generalization and sample efficiency.</a>
                </p>
				<center><image src="img/fig1-VLBiMan-overview.jpg" width="800px" class="img-responsive" alt="overview"></center>
				<p class="text-justify">
					<strong>Vision-Language Anchored Bimanual Manipulation (VLBiMan).</strong> <em>Left</em>: Taking pouring water as an example, we sketch the entire process of VLBiMan, including task-aware decomposition, novel scene adaptation, and autonomous trajectory composition, based on the one-shot demonstration. <em>Right</em>: Supported by VLBiMan, we have achieved generalizable bimanual manipulation on a variety of complex contact-rich tasks without retraining, robustly coping with diverse scenarios such as environmental changes, dynamic interference, and advanced tool use. </a>
                </p>
				<center><image src="img/fig2-VLBiMan-framework.jpg" width="800px" class="img-responsive" alt="overview"></center>
				<p class="text-justify">
					<strong>Detailed framework of VLBiMan.</strong> Taking the <em>pouring water</em> task as an example, we illustrate this paradigm consisting of three core stages (e.g., decomposition, adaptation, and composition) based on a given demonstration. By applying <strong>VLBiMan</strong>, we can achieve generalization of unseen spatial placements and category-level new instances under the same task.</a>
                </p>
				<center><image src="img/fig3-representative-points.jpg" width="800px" class="img-responsive" alt="overview"></center>
				<p class="text-justify">
					Illustrations of <strong>selecting representative points</strong> of manipulated objects in three increasingly difficult tasks: <em>pouring</em> (left), <em>reorient+unscrew</em> (middle) and <em>tool-use:spoon</em> (right). These representative points will be used to calculate the change in position and orientation of the newly placed object.</a>
                </p>
				
			</div>
        </div>



        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    ▶ Tasks, Implementation Details and Experimental Results
                </h3>
				<center><image src="img/fig4-object-assets-platform.jpg" width="850px" class="img-responsive" alt="overview"></center>
				<p class="text-justify">
					<strong>Top-Right:</strong> The fixed-base dual-arm manipulator platform (a table with two robot arms, two grippers and the binocular camera) used in this research. <strong>Left and Bottom:</strong> Manipulated object assets involved in all six bimanual manipulation tasks (including <em>plugpen</em>, <em>inserting</em>, <em>unscrew</em>, <em>pouring</em>, <em>pressing</em> and <em>reorient</em>) with primary skills and four long-horizon bimanual manipulation tasks (including <em>reorient+unscrew</em>, <em>unscrew+pouring</em>, <em>tool-use spoon</em> and <em>tool-use funnel</em>). Each object has been scaled down proportionally.</a>
                </p>
				<image src="img/gray_line.jpg" class="img-responsive" alt="overview">
				<center><image src="img/alg1-object-orientation-estimation.jpg" width="650px" class="img-responsive" alt="overview"></center>
                <p class="text-justify">
					<strong>Image Moments based Orientation Estimation.</strong> In the Vision-Language Anchored Adaptation pipeline of VLBiMan, our method requires extracting the principal axis and determining the orientation of direction-sensitive objects. This includes the marker pen in the <em>plugpen</em> and <em>inserting</em> tasks, the spoon in the <em>reorient</em> and <em>tool-use spoon</em> tasks, as well as the horizontally placed bottle in the <em>reorient+unscrew</em> task. As shown in <strong>Alg.1</strong>, we adopt an object principal axis extraction algorithm based on image moments theory. Since this algorithm relies primarily on the 2D segmentation mask of object and does not require any deep networks, its computational overhead is minimal and can be considered negligible in practice.</a>
                </p>
				<image src="img/gray_line.jpg" class="img-responsive" alt="overview">
                <p class="text-justify">
					<strong>Baselines and Metric.</strong> For each task setting, we conduct 20 trials, where objects are randomly located or replaced, and the success rate will be reported. For baselines, we compare to <strong>ReKep</strong> based on VFMs (<strong>SAM</strong> and <strong>DINOv2</strong>) and <strong>GPT-4o</strong>, as well as <strong>Robot-ABC</strong> based on keypoint affordance prediction with using <strong>AnyGrasp</strong> for initial grasping (After which, the remaining trajectory is obtained by trivial modules combination). </a>
                </p>
                <p class="text-justify">
					In experiments, we aim to answer following research questions: <strong>(1)</strong> <em>How well does our framework automatically formulate and synthesize bimanual manipulation behaviors?</em> <strong>(2)</strong> <em>Can our method generalize to novel scenarios and achieve effective combination of skills?</em> <strong>(3)</strong> <em>How do individual components contribute to the effectiveness and robustness of our system?</em> </a>
                </p>
				<center><image src="img/gray_line.jpg" class="img-responsive" width="700px" alt="overview"></center>
				<center><image src="img/tab1-quantitative-results-6tasks.jpg" width="700px" class="img-responsive" alt="overview"></center>
				<center><image src="img/gray_line.jpg" class="img-responsive" width="700px" alt="overview"></center>
				<center><image src="img/tab2-quantitative-results-4tasks.jpg" width="700px" class="img-responsive" alt="overview"></center>
				<center><image src="img/gray_line.jpg" class="img-responsive" width="700px" alt="overview"></center>
				<center><image src="img/tab3-ablation-fig6-failures.jpg" width="700px" class="img-responsive" alt="overview"></center>
				<center><image src="img/gray_line.jpg" class="img-responsive" width="700px" alt="overview"></center>
                <p class="text-justify">
					Above results have answered three questions raised earlier, demonstrating that VLBiMan can efficiently compose executable bimanual trajectories under diverse scene variations. Without reliance on object-specific priors or pose annotations, VLBiMan achieves robust generalization across unseen object instances and layouts. </a>
                </p>
				
            </div>
        </div>

		
		
        <div class="row">
    		<div class="col-md-10 col-md-offset-1">	
				<h3>
                    ▶ Visualization and Video Records of Real Robot Rollouts
                </h3>
				<center><image src="img/fig5-rollouts-visualization.jpg" width="800px" class="img-responsive" alt="overview"></center>
                <p class="text-justify">
					Above is the visualization of all ten tasks executed on real robots. They are designed to validate different aspects, including <strong>ⓐ</strong> six dual-arm primary skills, <strong>ⓑ</strong> combination of basic skills for two long-horizon tasks, and <strong>ⓒ</strong> exploration of multi-stage spatiotemporal dependencies in two tool-use tasks. </a>
                </p>
				<image src="img/gray_line.jpg" class="img-responsive" alt="overview">
                <p class="text-justify">
					<strong>Video collection of real robot deployments (with human dynamic interference) for all ten tasks<strong>. The video central area shows manipulated objects involved in each task. Ten tasks include <em>plugpen</em>, <em>inserting</em>, <em>unscrew</em>, <em>pouring</em>, <em>pressing</em>, <em>reorient</em>, <em>reorient+unscrew</em>, <em>unscrew+pouring</em>, <em>tool-use spoon</em> and <em>tool-use funnel</em>. </a>
                </p>
				<table style="width: 100%; border-collapse: collapse;">
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t01_plugpen_all_4_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t02_inserting_all_8_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t03_unscrew_all_8_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t04_pouring_all_8_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t05_pressing_all_4_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t06_reorient_all_4_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t07_reorient_unscrew_all_4_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t08_unscrew_pouring_all_4_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t09_tool_spoon_all_4_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/t10_tool_funnel_all_4_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
				</table>

                <p class="text-justify">
					<strong>Good robustness of VLBiMan to lighting changes.<strong> Below videos comparison reveals the impact of uneven lighting on the robustness of six basic bimanual tasks (still with interference). As can be seen, our solution VLBiMan combined with VLMs is robust to uneven lighting.</a>
				</p>
				<center><image src="img/tabA1-quantitative-uneven-light.jpg" width="700px" class="img-responsive" alt="overview"></center>
				<table style="width: 100%; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">Even Light Results</td>
						<td style="text-align: center;">Uneven Light Results</td>
					</tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/Light_Even-6_tasks_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/Light_Uneven-6_tasks_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
				</table>



                <p class="text-justify">
					<strong>Efficient synchronous dual-arm movement<strong> Below is comparison of the time consumption for asynchronous or synchronous execution of all ten tasks. We also created a dynamic comparison chart of all ten tasks into one page slice for quick viewing. The coordination and synchronization of two arms can obviously improve manipulation efficiency. We observed time savings of varying magnitudes across all ten tasks, yielding <em>an average improvement in execution efficiency of approximately 22%</em>.</a>
				</p>
				<table style="width: 100%; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">plugpen</td>
						<td style="text-align: center;">inserting</td>
						<td style="text-align: center;">unscrew</td>
						<td style="text-align: center;">pouring</td>
						<td style="text-align: center;">pressing</td>
						<td style="text-align: center;">reorient</td>
					</tr>
					<tr>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-plugpen_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-inserting_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-unscrew_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-pouring_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-pressing_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-reorient_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: center;"></td>
						<td style="text-align: center;">reorient+unscrew</td>
						<td style="text-align: center;">unscrew+pouring</td>
						<td style="text-align: center;">tool-use spoon</td>
						<td style="text-align: center;">tool-use funnel</td>
						<td style="text-align: center;"></td>
					</tr>
					<tr>
						<td style="text-align: center;"></td>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-reorient_unscrew_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-unscrew_pouring_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-tool_spoon_3x_slim.mp44" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="155px" autoplay loop muted>
						  <source src="videos/Asyn_vs_Syn-tool_funnel_3x_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;"></td>
					</tr>
				</table>
				<center><image src="img/time-cost-stat.jpg" width="800px" class="img-responsive" alt="overview"></center>
				
			</div>
        </div>
		
		
					
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{zhou2025vision,
	title={Vision-Language Anchored One-Shot Demonstration Enables Generalizable Bimanual Manipulation},
	author={Huayi Zhou, Kui Jia},
	journal={arXiv preprint arXiv:2507.xxxxxx},
	year={2025}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We acknowledge the providers of various hardware used in this project, including the <a href="https://www.aubo-cobot.com/public/i5product3">Aubo-i5 robotic arm</a>, <a href="https://en.dh-robotics.com/product/pgi">DH gripper PGI-80-80</a>, and <a href="https://dexforce-3dvision.com/productinfo/1022811.html">kingfisher binocular camera</a>. 
                    <br><br>
                The website template was borrowed from <a href="https://jonbarron.info/">Jon Barron</a> and <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>.
                </p>
            </div>
        </div>
		
    </div>
</body>
</html>
