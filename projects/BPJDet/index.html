
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>BPJDet</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://hnuzhy.github.io/projects/BPJDet/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://hnuzhy.github.io/projects/BPJDet/"/>
    <meta property="og:title" content="BPJDet: Extended Object Representation for Generic Body-Part Joint Detection" />
    <meta property="og:description" content="Detection of human body and its parts (e.g., head or hands) has been intensively studied. However, most of these CNNs-based detectors are trained independently, making it difficult to associate detected parts with body. In this paper, we focus on the joint detection of human body and its corresponding parts. Specifically, we propose a novel extended object representation integrating center-offsets of body parts, and construct a dense one-stage generic Body-Part Joint Detector (BPJDet). In this way, body-part associations are neatly embedded in a unified object representation containing both semantic and geometric contents. Therefore, we can perform multi-loss optimizations to tackle multi-tasks synergistically. BPJDet does not suffer from error-prone post matching, and keeps a better trade-off between speed and accuracy. Furthermore, BPJDet can be generalized to detect any one or more body parts. To verify the superiority of BPJDet, we conduct experiments on three body-part datasets (CityPersons, CrowdHuman and BodyHands) and one body-parts dataset COCOHumanParts. While keeping high detection accuracy, BPJDet achieves state-of-the-art association performance on all datasets comparing with its counterparts. Besides, we show benefits of advanced body-part association capability by improving performance of two representative downstream applications: accurate crowd head detection and hand contact estimation."/>



<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>âš¡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>BPJDet</b>: Extended Object Representation for Generic Body-Part Joint Detection</br> 
                <small>
                arXiv 2023 (accepted by TPAMI in 2024.01)
                </small></br>
                <small><small>
                The conference verison named "Body-Part Joint Detection and Association via Extended Object Representation" has obtained the "ICME 2023 Best Student Paper Runner Up Award"
                </small></small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://hnuzhy.github.io">Huayi Zhou</a>
                        </br>Shanghai Jiao Tong University (SJTU)
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ&hl=en">Fei Jiang</a>
                        </br>East China Normal University (ECNU)
                    </li>
                    <li><br>
                        <a>Jiaxin Si</a>
                        </br>Chongqing Qulian Digital Technology Company
                    </li>
                    <li>
                        <a href="https://www.cs.sjtu.edu.cn/PeopleDetail.aspx?id=469">Yue Ding</a>
                        </br> Shanghai Jiao Tong University (SJTU)
                    </li>
                    <li>
                        <a href="https://www.cs.sjtu.edu.cn/PeopleDetail.aspx?id=95">Hongtao Lu</a>
                        </br> Shanghai Jiao Tong University (SJTU)
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-10 col-md-offset-1 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://ieeexplore.ieee.org/document/10400895">
                            <image src="img/BPJDetPlus_image.jpg" height="40px">
                                <h4><strong>Paper(Journal)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2304.10765">
                            <image src="img/BPJDetPlus_image.jpg" height="40px">
                                <h4><strong>arXiv(Journal)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/hnuzhy/BPJDet/tree/BPJDetPlus">
                            <image src="img/github.png" height="40px">
                                <h4><strong>Code(Journal)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://ieeexplore.ieee.org/document/10219738">
                            <image src="img/BPJDet_image.jpg" height="40px">
                                <h4><strong>Paper(Conference)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2212.07652">
                            <image src="img/BPJDet_image.jpg" height="40px">
                                <h4><strong>arXiv(Conference)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/hnuzhy/BPJDet">
                            <image src="img/github.png" height="40px">
                                <h4><strong>Code(Conference)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://news.sjtu.edu.cn/jdyw/20230809/186979.html">
                            <image src="img/news_sjtu.jpg" height="40px">
                                <h4><strong>News</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
		  <source src="img/teaser.mp4" type="video/mp4" />
                </video>
	    </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/illustration.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
Detection of human body and its parts (e.g., head or hands) has been intensively studied. However, most of these CNNs-based detectors are trained independently, making it difficult to associate detected parts with body. In this paper, we focus on the joint detection of human body and its corresponding parts. Specifically, we propose a novel extended object representation integrating center-offsets of body parts, and construct a dense one-stage generic Body-Part Joint Detector (BPJDet). In this way, body-part associations are neatly embedded in a unified object representation containing both semantic and geometric contents. Therefore, we can perform multi-loss optimizations to tackle multi-tasks synergistically. BPJDet does not suffer from error-prone post matching, and keeps a better trade-off between speed and accuracy. Furthermore, BPJDet can be generalized to detect any one or more body parts. To verify the superiority of BPJDet, we conduct experiments on three body-part datasets (CityPersons, CrowdHuman and BodyHands) and one body-parts dataset COCOHumanParts. While keeping high detection accuracy, BPJDet achieves state-of-the-art association performance on all datasets comparing with its counterparts. Besides, we show benefits of advanced body-part association capability by improving performance of two representative downstream applications: accurate crowd head detection and hand contact estimation.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BPJDet vs. BPJDetPlus
                </h3>
				<table style="width: 100%; border-collapse: collapse;">
				  <tr>
				    <td style="text-align: center;">
		                <video id="v0" width="100%" autoplay loop muted>
		                  <source src="img/BPJDet-bodyhead.mp4" type="video/mp4" />
		                </video>
					</td>
				    <td style="text-align: center;">
		                <video id="v0" width="100%" autoplay loop muted>
		                  <source src="img/BPJDetPlus-bodyparts.mp4" type="video/mp4" />
		                </video>
					</td>
				  </tr>
				  <tr>
				    <td style="text-align: center;">BPJDet (body-part)</td>
				    <td style="text-align: center;">BPJDetPlus (body-parts)</td>
				  </tr>
                  <image src="img/BPJDet_BPJDetPlus.png" class="img-responsive" alt="overview"><br>
				</table>
                <p class="text-justify">
                    BPJDetPlus is newly added in the journal version of BPJDet. It has various new functions: (1) Multiple Body-Parts Joint Detectionm; (2) Two downstream applications including Body-Head for Accurate Crowd Counting and Body-Hand for Hand Contact Estimation.
                </p>
            </div>
        </div>
	    
<br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    More Quantitative Results of BPJDet and BPJDetPlus
                </h3>
                <p style="text-align:center;"><image src="img/MoreResults.jpg" width=100% alt="overview"></p><br>
                <p class="text-justify">
                    We show more result visualization examples of trained BPJDet-L on four different joint detection tasks: <em>body-face</em>, <em>body-hand</em>, <em>body-head</em> and <em>body-parts</em> respectively. The <em>body-face</em> and <em>body-head</em> tasks are trained on the train-set of <a href="https://www.crowdhuman.org/">CrowdHuman</a>. They perform satisfactory detection and association capabilities on images collected in-the-wild, showing excellent adaptability. The <em>body-hand</em> task is trained on the train-set of <a href="http://vision.cs.stonybrook.edu/~supreeth/BodyHands/">BodyHands</a>. Although the model does not achieve body detection results as good as trained on CrowdHuman due to the sparse and incomplete labels of the BodyHands dataset, it can still accurately match the detected hands to their body boxes thanks to the superior association ability of our BPJDet. The <em>body-parts</em> task is trained on the <a href="https://github.com/soeaver/Hier-R-CNN">COCOHumanParts</a>. We can observe that subtasks including <em>body-face</em>, <em>body-hand</em> and <em>body-head</em> are all included and unified in one model which exhibits impressive results. This profits from the annotation richness and data diversity of benchmark, while is also inseparable from the state-of-the-art power of BPJDet.</a>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Application 1: Body-Head for Accurate Crowd Counting
                </h3>
                <video class="video" width=100% id="xyComparing" loop playsinline autoplay muted src="img/SCUTHeadDet_demo.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas height=0 class="videoMerge" id="xyComparingMerge"></canvas>
                <p class="text-justify">
                    Through the joint detection of human body and head, false detections of single-object detectors are eliminated, and accurate detection and counting of crowded persons are achieved without retraining. The detection effect is robust and can automatically adapt to unfamiliar scenes and various closed environments, including classrooms, conference rooms, shopping malls, etc. The used images are all from the dataset <a href="https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release">SCUT-HEAD PartB</a>
                </p>
            </div>
        </div>
	    
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Application 2: Body-Hand for Hand Contact Estimation
                </h3>
                <p style="text-align:center;"><image src="img/ContactHands.png" width=100% alt="overview"></p><br>
                <p class="text-justify">
                    The purpose of this task is to detect human hands in a natural state and estimate their physical contact status, which is divided into the following four categories: â‘  No-Contact: the hand is not in contact with any object; â‘¡ Self-Contact: The hand comes into contact with certain parts of the human body itself; â‘¢ Contact with others (Person-Contact): the hand comes into contact with other human body parts that are not the person's own; â‘£ Contact with the object (Object-Contact): the hand comes into contact with non-human body objects. And the state of the hand may be a superposition of the above categories. For example, someone's hand can be holding an object while touching another person. Human hand contact state estimation can be used to support a variety of applications, such as harassment detection, pollution prevention, behavior recognition or AR/VR, etc. More details can be found in <a href="https://vision.cs.stonybrook.edu/~supreeth/ContactHands_data_website/">ContactHands(NIPS2020) Detecting Hands and Recognizing Physical Contact in the Wild</a>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{zhou2024bpjdet,
  title={BPJDET: extended object representation for generic body-part joint detection},
  author={Zhou, Huayi and Jiang, Fei and Si, Jiaxin and Ding, Yue and Lu, Hongtao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={46},
  number={6},
  pages={4314--4330},
  year={2024},
  publisher={IEEE}
}			    
@inproceedings{zhou2023body,
  title={Body-part joint detection and association via extended object representation},
  author={Zhou, Huayi and Jiang, Fei and Lu, Hongtao},
  booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={168--173},
  year={2023},
  organization={IEEE}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We acknowledge the effort from authors of human-related datasets including <a href="https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons">CityPersons</a>, <a href="https://www.crowdhuman.org/">CrowdHuman</a>, <a href="http://vision.cs.stonybrook.edu/~supreeth/BodyHands/">BodyHands</a>, <a href="https://github.com/soeaver/Hier-R-CNN">COCOHumanParts</a>, <a href="https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release">SCUT-HEAD PartB</a>, <a href="https://motchallenge.net/data/Head_Tracking_21/">CroHD</a> and <a href="https://vision.cs.stonybrook.edu/~supreeth/ContactHands_data_website/">ContactHands</a>. These datasets make researches and downstream applications about generic body-part joint detection and association possible. This paper was supported by NSFC (No. 62176155, 62207014), Shanghai Municipal Science and Technology Major Project, China, under grant no. 2021SHZDZX0102. 
                    <br><br>
                The website template was borrowed from <a href="https://jonbarron.info/">Jon Barron</a> and <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>

