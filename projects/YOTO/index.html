
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>YOTO</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://hnuzhy.github.io/projects/BPJDet/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://hnuzhy.github.io/projects/YOTO/"/>
    <meta property="og:title" content="You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations" />
    <meta property="og:description" content="Bimanual robotic manipulation is a long-standing challenge of embodied intelligence due to its characteristics of dual-arm spatial-temporal coordination and high-dimensional action spaces. Previous studies rely on pre-defined action taxonomies or direct teleoperation to alleviate or circumvent these issues, often making them lack simplicity, versatility and scalability. Differently, we believe that the most effective and efficient way for teaching bimanual manipulation is learning from human demonstrated videos, where rich features such as spatial-temporal positions, dynamic postures, interaction states and dexterous transitions are available almost for free. In this work, we propose the YOTO (You Only Teach Once), which can extract and then inject patterns of bimanual actions from as few as a single binocular observation of hand movements, and teach dual robot arms various complex tasks. Furthermore, based on keyframes-based motion trajectories, we devise a subtle solution for rapidly generating training demonstrations with diverse variations of manipulated objects and their locations. These data can then be used to learn a customized bimanual diffusion policy (BiDP) across diverse scenes. In experiments, YOTO achieves impressive performance in mimicking 5 intricate long-horizon bimanual tasks, possesses strong generalization under different visual and spatial conditions, and outperforms existing visuomotor imitation learning methods in accuracy and efficiency."/>

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations</br> 
                <small> arXiv 2025.01 (accepted by <strong>Robotics: Science and Systems (RSS) 2025</strong>) </small></br>
		<small> Please see our extended journal version <a href="https://hnuzhy.github.io/projects/YOTOPlus/">YOTO++</a> for more new content </small></br>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://hnuzhy.github.io">Huayi Zhou</a>
                        </br>The Chinese University of Hong Kong, Shenzhen
                    </li>
                    <li>
                        <a href="https://openreview.net/profile?id=~Ruixiang_Wang3">Ruixiang Wang</a>
                        </br>Harbin Institute of Technology, Weihai
                    </li>
                    <li>
                        <a href="https://openreview.net/profile?id=~Yunxin_Tai2">Yunxin Tai</a>
                        </br>DexForce, Shenzhen
                    </li>
                    <li><br>
                        <a href="https://github.com/yuecideng">Yueci Deng</a>
                        </br>DexForce, Shenzhen
                    </li>
                    <li>
                        <a href="http://guiliang.me/">Guiliang Liu</a>
                        </br>The Chinese University of Hong Kong, Shenzhen
                    </li>
                    <li>
                        <a href="http://kuijia.site">Kui Jia</a>
                        </br>The Chinese University of Hong Kong, Shenzhen
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://www.roboticsproceedings.org/rss21/p149.pdf">
                            <image src="img/paper.png" height="50px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2501.14208">
                            <image src="img/logo-arXiv-black-rmbg.png" height="50px">
                                <h4><strong>arXiv</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://huggingface.co/HoyerChou/YOTO">
                            <image src="img/dataset.png" height="50px">
                                <h4><strong>Dataset</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/hnuzhy/YOTO">
                            <image src="img/github.png" height="50px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
				<source src="video/YOTO_Auto-Rollout_Verification.mp4" type="video/mp4" />
                </video>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Bimanual robotic manipulation is a long-standing challenge of embodied intelligence due to its characteristics of dual-arm spatial-temporal coordination and high-dimensional action spaces. Previous studies rely on pre-defined action taxonomies or direct teleoperation to alleviate or circumvent these issues, often making them lack simplicity, versatility and scalability. Differently, we believe that the most effective and efficient way for teaching bimanual manipulation is learning from human demonstrated videos, where rich features such as spatial-temporal positions, dynamic postures, interaction states and dexterous transitions are available almost for free. In this work, we propose the YOTO (You Only Teach Once), which can extract and then inject patterns of bimanual actions from as few as a single binocular observation of hand movements, and teach dual robot arms various complex tasks. Furthermore, based on keyframes-based motion trajectories, we devise a subtle solution for rapidly generating training demonstrations with diverse variations of manipulated objects and their locations. These data can then be used to learn a customized bimanual diffusion policy (BiDP) across diverse scenes. In experiments, YOTO achieves impressive performance in mimicking 5 intricate long-horizon bimanual tasks, possesses strong generalization under different visual and spatial conditions, and outperforms existing visuomotor imitation learning methods in accuracy and efficiency.
                </p>
            </div>
        </div>


        <div class="row">
    		<div class="col-md-8 col-md-offset-2">
				<h3>
                    ▶ Details and Framework of Our YOTO
                </h3>
                <p class="text-justify">
                    Our proposed YOTO (You Only Teach Once) facilitates various complex long-horizon bimanual tasks. It needs only a one-shot observation of a single third-person binocular camera to extract the fine-grained motion trajectory of human hands, which can then be utilized for the dual-arm coordinated action injection and rapid proliferation of training demonstrations.</a>
                </p>
				<image src="img/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    The overview of our proposed YOTO. It is a general framework consists of three main modules: (a) the human hand motion extraction and injection, (b) the training demonstration proliferation from one-shot teaching, and (c) the training and deployment of a customized bimanual diffusion policy (BiDP). It is best to zoom in to view the details.</a>
                </p>
				<image src="img/framework.png" class="img-responsive" alt="overview"><br>
			</div>
        </div>


        <div class="row">
    		<div class="col-md-8 col-md-offset-2">	
				<h3>
                    ▶ Hand Motion Extraction and Injection
                </h3>
                <p class="text-justify">
		We focus on understanding human hands, including their location, left-rightness, 3D shape, joints, pose, contact, and open/closed state. These features can be perceived using hand-related vision methods. After extracting hand motion trajectories, we do not simply inject step-wise actions into robots, but choose to simplify the consecutive trajectory into discrete keyframes, and assign the corresponding keyposes to two arms to execute by applying inverse kinematics interpolation. Besides, we also record and replay the order of dual-hand movements (termed as <strong>motion mask</strong>), which can help to address the dual-arm coordination issue in long-horizon bimanual tasks. Now, we successfully obtain a stable and refined manipulation motion exemplar.</a>
                </p>
                <p class="text-justify">
		Below is illustrations of five major bimanual tasks: <strong>pull drawer</strong>, <strong>pour water</strong>, <strong>unscrew bottle</strong>, <strong>uncover lid</strong> and <strong>open box</strong>.</a>
				</p>
				<table style="width: 100%; border-collapse: collapse;">
					<tr><td style="text-align: left;">Pull Drawer</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t1_drawer_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t1_drawer_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t1_drawer_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Pour Water</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t2_pouring_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t2_pouring_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t2_pouring_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Unscrew Bottle</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t3_unscrew_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t3_unscrew_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t3_unscrew_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Uncover Lid</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t4_uncover_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t4_uncover_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t4_uncover_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Open Box</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t5_openbox_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t5_openbox_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t5_openbox_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
				</table>
                <p class="text-justify">
					Another two new bimanual tasks <strong>reorient pen</strong> and <strong>flip basket</strong> are also demonstrated. These extracted actions can be injected into real dual-arm robots quickly and easily through one-shot human teaching. These results further reveal the simplicity, versatility and scalability of YOTO. Due to space limitations, we did not continue the demonstration proliferation and policy training experiments.</a>
				</p>
				<table style="width: 100%; border-collapse: collapse;">
					<tr><td style="text-align: left;">Reorient Pen</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t6_redirect_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t6_redirect_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t6_redirect_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Flip Basket</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t7_invert_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t7_invert_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t7_invert_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
				</table>
                <p class="text-justify">
					Recently (2025.02), we invited three volunteers to demonstrate with doing their randomly imagined bimanual tasks (including <strong>reorient calib board</strong>, <strong>plug mug pen</strong> and <strong>stack mug cups</strong>). These extracted actions can also be injected into real dual-arm robots quickly and easily. This suggests that our method is broadly applicable to most humans.</a>
				</p>
				<table style="width: 100%; border-collapse: collapse;">
					<tr><td style="text-align: left;">Reorient Calib Board</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t8_reorient_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t8_reorient_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t8_reorient_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Plug Mug Pen</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t9_plugpen_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t9_plugpen_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t9_plugpen_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Stack Mug Cups</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t10_stacking_raw_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t10_stacking_vis_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/t10_stacking_robot.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
				</table>
			</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    ▶ Auto-Rollout Verification in Real-World
                </h3>
                <p class="text-justify">
                    Based on the one-shot teaching, we propose two demonstration proliferation schemes, the automatic rollout verification of real robots and point cloud-level geometry augmentation of manipulated objects. This solution is an efficient and reliable route to quickly produce training data for imitation learning. Below is the example showing how to conduct the automatic rollout verification (including category variations and location variations).</a>
                </p>
                <p class="text-justify">
		    Before that, when doing the automatic rollout verification, we claim that all manipulated objects can be replaced with similar shapes in the same position to expand category diversity. Therefore, we collected a variety of manipulated objects in instance-level for each of five bimanual tasks to improve and verify the generalizability of trained policies. All of these objects are from everyday life, not intentionally customized.</a>
                </p>
				<image src="img/assets.png" class="img-responsive" alt="overview"><br>
				<table style="width: 100%; border-collapse: collapse;">
					<tr><td style="text-align: left;">Pull Drawer (3 for category variations, 1 for location variation)</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_drawer_category_v1.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_drawer_category_v2.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_drawer_category_v3.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_drawer_location.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Pour Water (3 for category variations, 1 for location variation)</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_pouring_category_v1.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_pouring_category_v2.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_pouring_category_v3.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_pouring_location.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Unscrew Bottle (1 for category variation, 1 for location variation)</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_unscrew_category.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_unscrew_location.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Uncover Lid (1 for category variation, 1 for location variation)</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_uncover_category.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_uncover_location.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Open Box (1 for category variation, 1 for location variation)</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_openbox_category.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="372px" autoplay loop muted>
						  <source src="video/autoRollout_openbox_location.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
				</table>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    ▶ Qualitative Evaluation Results of BiDP
                </h3>
		<p class="text-justify">
		We evaluate YOTO on five real-world bimanual tasks, including <strong>pull drawer</strong>, <strong>pour water</strong>, <strong>unscrew bottle</strong>, <strong>uncover lid</strong> and <strong>open box</strong>. These tasks collectively encompass two types of dual-arm collaborations: strictly asynchronous and synchronous. The manipulated objects in these tasks might be rigid, articulated, deformable or non-prehensile. They also involve many primitive skills such as pull/push, pick/place, re-orient, unscrew, revolve and lift up. Some skills must require both arms to complete. More importantly, all tasks are long-horizon, indicating that they are quite complex due to containing multiple substeps. </a>
                </p>
                <p class="text-justify">
                Below, we show qualitative rollout samples from the third-person perspective for all evaluation tasks we mentioned in the main paper. We can observe our model’s generalization to object category and location variations. These examples show more complete scenes and the motion of two robot arms, and can be considered as a supplement to the limited field of view of the binocular observation camera. Note that these third-person video recordings do not participate in any training and testing.</a>
                </p>
				<tr><td style="text-align: center;">Pull Drawer</td></tr>
				<video id="v0" width="100%" autoplay loop muted>
					<source src="video/eval_drawer_slim.mp4" type="video/mp4" />
                </video>
				<tr><td style="text-align: center;">Pour Water</td></tr>
				<video id="v0" width="100%" autoplay loop muted>
					<source src="video/eval_pouring_slim.mp4" type="video/mp4" />
                </video>
				<tr><td style="text-align: center;">Unscrew Bottle</td></tr>
				<video id="v0" width="100%" autoplay loop muted>
					<source src="video/eval_unscrew_slim.mp4" type="video/mp4" />
                </video>
				<tr><td style="text-align: center;">Uncover Lid</td></tr>
				<video id="v0" width="100%" autoplay loop muted>
					<source src="video/eval_uncover_slim.mp4" type="video/mp4" />
                </video>
				<tr><td style="text-align: center;">Open Box</td></tr>
				<video id="v0" width="100%" autoplay loop muted>
					<source src="video/eval_openbox_slim.mp4" type="video/mp4" />
                </video>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    ▶ Failure Cases
                </h3>
                <p class="text-justify">
                    Although our method BiDP outperforms many strong baselines for addressing long-horizon bimanual manipulation tasks, it still presents various failure cases during evaluation. Below, we focus our analysis on failure of BiDP in real-world experiments, and show some representative failure examples of all real robot executions we have performed with our method.</a>
                </p>
				<table style="width: 100%; border-collapse: collapse;">
					<tr><td style="text-align: left;">Pull Drawer</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0243-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0242-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0241-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Pour Water</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0240-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0239-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0238-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Unscrew Bottle</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0236-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0235-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0234-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Uncover Lid</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0233-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0232-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0231-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">Open Box</td></tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0230-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: center;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0229-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="245px" autoplay loop muted>
						  <source src="video/C0228-Trim_slim.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
				</table>
            </div>
        </div>
					
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{zhou2025you,
	title={You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations},
	author={Huayi Zhou, Ruixiang Wang, Yunxin Tai, Yueci Deng, Guiliang Liu, Kui Jia},
	journal={arXiv preprint arXiv:2501.14208},
	year={2025}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We acknowledge the providers of various hardware used in this project, including the <a href="https://www.aubo-cobot.com/public/i5product3">Aubo-i5 robotic arm</a>, <a href="https://en.dh-robotics.com/product/pgi">DH gripper PGI-80-80</a>, and <a href="https://dexforce-3dvision.com/productinfo/1022811.html">kingfisher binocular camera</a>. 
                    <br><br>
                The website template was borrowed from <a href="https://jonbarron.info/">Jon Barron</a> and <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>

