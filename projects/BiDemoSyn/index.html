
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>BiDemoSyn</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://hnuzhy.github.io/projects/BPJDet/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://hnuzhy.github.io/projects/BiDemoSyn/"/>
    <meta property="og:title" content="One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation" />
    <meta property="og:description" content="Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding."/>



<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation</br> 
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://hnuzhy.github.io">Huayi Zhou</a>
                        </br>The Chinese University of Hong Kong, Shenzhen
                    </li>
                    <li>
                        <a href="http://kuijia.site">Kui Jia</a>
                        </br>The Chinese University of Hong Kong, Shenzhen; DexForce, Shenzhen
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="BiDemoSyn.pdf">
                            <image src="img/logo_document.png" height="50px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
						<li>
                            <a href="https://arxiv.org/abs/2511.xxxxx">
                            <image src="img/logo_arXiv.png" height="50px">
                                <h4><strong>arXiv</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/hnuzhy/">
                            <image src="img/logo_github.png" height="50px">
                                <h4><strong>Code (coming soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <video id="v0" width="100%" autoplay loop muted controls>
				<source src="videos/teaserBiDemoSynSlim.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
				<source src="videos/teaserVideoSlim.mp4" type="video/mp4" />
                </video>
			</div>
        </div>


        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
					Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.
                </p>
            </div>
        </div>


        <div class="row">
    		<div class="col-md-10 col-md-offset-1">
				<h3>
                    ▶ Framework and Overview of BiDemoSyn
                </h3>
                <p class="text-justify">
					Our contributions about the proposed BiDemoSyn are threefold: <strong>(1) One-Shot Synthesis Framework:</strong> A systematic pipeline combining task decomposition, vision-guided adaptation, and contact-aware trajectory optimization to generate scalable real-world bimanual demonstrations. <strong>(2) Reality-Grounded Data Generation:</strong> A completely simulator-free method for synthesizing bimanual demonstrations, ensuring physical fidelity by construction. <strong>(3) Empirical Validation in Complex Tasks:</strong> Comprehensive real-robot experiments demonstrating significant improvements in policy robustness and cross-configuration generalization on various bimanual manipulation tasks.</a>
                </p>
				<center><image src="img/fig1-from-one-to-many.jpg" width="800px" class="img-responsive" alt="overview"></center>
				<p class="text-justify">
					<strong>From One to Many ①⭢Ⓝ.</strong> Taking the example of dual-arm coordinated <em>pouring</em> task, we illustrate how to synthesize corresponding pre-grasping and lifting trajectories for different new placements and novel instances of manipulated objects during the initial frame alignment phase.</a>
                </p>
				<center><image src="img/fig2-overview-of-bidemosyn.jpg" width="800px" class="img-responsive" alt="overview"></center>
				<p class="text-justify">
					<strong>The overview of BiDemoSyn.</strong> It consists of three stages (e.g., deconstruction, alignment, and optimization) based on a given demonstration. Then, we can apply our method to complete data collection efficiently and conveniently in real-world. It is best to zoom in to view the details.</a>
                </p>
				<center><image src="img/fig3-initial-frame-alignment.jpg" width="800px" class="img-responsive" alt="overview"></center>
				<p class="text-justify">
					Illustrations of the <strong>initial frame alignment stage</strong> applied to tasks <em>pouring</em> (left and middle) and <em>reorient</em> (right). It shows that we can automatically adjust the grasp pose after the position, orientation and shape of the manipulated object changes.</a>
                </p>
				
			</div>
        </div>



        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    ▶ ① Implementation of Data Collection, Processing and Synthesis
                </h3>
				<center><image src="img/fig4-fig5-platform-gridcells.jpg" width="900px" class="img-responsive" alt="overview"></center>
				<p class="text-justify">
					<strong>Left-Top:</strong> The fixed-base dual-arm manipulator platform (a table with two robot arms, two grippers and the binocular camera) used in this research. <strong>Left-Bottom:</strong> Object assets involved in all six bimanual manipulation tasks. Each object has been scaled down proportionally. <strong>Right:</strong> The specific grid cell division way for each task. For tasks involving two manipulated objects, the total number of grid cells will be divided equally between the left side and right side.</a>
                </p>
				<table style="width: 100%; border-collapse: collapse;">
					<tr><td style="text-align: left;">task 1: <strong>plugpen</strong></td></tr>
					<tr>
						<td style="text-align: left;">
						<image src="img/UI-T1-plugpen-marker02.jpg" class="img-responsive" width="312px" alt="overview">
						</td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t1-plugpen-e1.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t1-plugpen-e2.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;"></td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t1-plugpen-e3.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t1-plugpen-e4.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">task 2: <strong>inserting</strong></td></tr>
					<tr>
						<td style="text-align: left;">
						<image src="img/UI-T2-inserting-ordcup01-marker02.jpg" class="img-responsive" width="312px" alt="overview">
						</td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t2-inserting-e1.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t2-inserting-e2.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;"></td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t2-inserting-e3.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t2-inserting-e4.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">task 3: <strong>unscrew</strong></td></tr>
					<tr>
						<td style="text-align: left;">
						<image src="img/UI-T3-unscrew-bottle03.jpg" class="img-responsive" width="312px" alt="overview">
						</td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t3-unscrew-e1.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t3-unscrew-e2.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;"></td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t3-unscrew-e3.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t3-unscrew-e4.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">task 4: <strong>pouring</strong></td></tr>
					<tr>
						<td style="text-align: left;">
						<image src="img/UI-T4-pouring-bottle03-mugcup01.jpg" class="img-responsive" width="312px" alt="overview">
						</td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t4-pouring-e1.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t4-pouring-e2.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;"></td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t4-pouring-e3.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t4-pouring-e4.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">task 5: <strong>pressing</strong></td></tr>
					<tr>
						<td style="text-align: left;">
						<image src="img/UI-T5-pressing-ordcup01-nozzle01.jpg" class="img-responsive" width="312px" alt="overview">
						</td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t5-pressing-e1.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t5-pressing-e2.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;"></td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t5-pressing-e3.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t5-pressing-e4.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr><td style="text-align: left;">task 6: <strong>reorient</strong></td></tr>
					<tr>
						<td style="text-align: left;">
						<image src="img/UI-T6-reorient-anyobj01.jpg" class="img-responsive" width="312px" alt="overview">
						</td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t6-reorient-e1.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t6-reorient-e2.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;"></td>
						<td style="text-align: center;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t6-reorient-e3.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="312px" autoplay loop muted>
						  <source src="videos/BiDemoSyn-t6-reorient-e4.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
				</table>
                <p class="text-justify">
					<strong>UI interface</strong> and <strong>real-world data collection</strong> examples. We developed a convenient UI interface for quick collection of diverse observation images. In the interface, we can check whether a grid cell is sampled or not for the corresponding object. When collecting data, the six tasks need to display different grid cells in real time, as well as perceive and track objects related to the tasks. Note that these points, crosses and lines are drawn digitally, which are not marks in the real world. All related videos are only accelerated 2 times, which can still reflect the high collection efficiency. </a>
                </p>
				
				<center><image src="img/fig-teaser-from-one-to-many.jpg" width="900px" class="img-responsive" alt="overview"></center>
				<p class="text-justify">
					<strong>From One to Many ①⭢Ⓝ.</strong> <em>Left:</em> Six representative bimanual manipulation tasks with their one-shot demonstrations and task-specific descriptors. <em>Right:</em> Real-world data collection diagrams, showing object instances with varied geometries and spatial arrangements used to synthesize diverse demonstrations (e.g., thousands physically consistent trajectories per task). </a>
                </p>
				
				
            </div>
        </div>



        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    ▶ ② Training, Deployment and Evaluation of Imitation Policies
                </h3>
                <p class="text-justify">
					We adapt two advanced visuomotor policies <a href="https://3d-diffusion-policy.github.io/">3D Diffusion Policy(DP3)</a> and <a href="https://equi-bot.github.io/">EquiBot</a> to bimanual settings by modifying their modeling spaces to dual-arm actions. Observation inputs are segmented 3D point clouds of task-relevant objects using <a href="https://huggingface.co/microsoft/Florence-2-large">Florence2</a> + <a href="https://github.com/facebookresearch/sam2">SAM2</a>, and policies operate in the open-loop discrete keyposes prediction to align with our synthesized demonstration format. <br>
					Compared baseline methods include two categories. One category is <em>purely for data collection</em> including point cloud editing <a href="https://demo-generation.github.io/">DemoGen</a>, real robot auto-rollout <a href="https://hnuzhy.github.io/projects/YOTO/">YOTO</a>, and human drag teaching (close to <strong>Teleoperation</strong>). Generally, the quality of collected demonstrations by these baselines is better in turn, but the cost is more time-consuming. The another category for comparing is <em>directly for bimanual manipulation without retraining</em> including the zero-shot <a href="https://rekep-robot.github.io/">ReKep</a>, an advanced ReKep+ with oracle-level grasp labels at the beginning, and one-shot <a href="https://www.robot-learning.uk/one-shot-dual-arm">ODIL</a>. </a>
                </p>
                <p class="text-justify">
					Our experiments address three core questions. <strong>Q1</strong>: <em>Is BiDemoSyn truly efficient and user-friendly?</em> <strong>Q2</strong>: <em>Does BiDemoSyn enable scalable visuomotor imitation learning?</em> <strong>Q3</strong>: <em>Does synthetic demonstrations generalize to spatial and object variations?</em> </a>
                </p>
				<center><image src="img/gray_line.jpg" class="img-responsive" width="700px" alt="overview"></center>
				<center><image src="img/fig6-efficiency-comparison.jpg" width="700px" class="img-responsive" alt="overview"></center>
				<center><image src="img/gray_line.jpg" class="img-responsive" width="700px" alt="overview"></center>
				<center><image src="img/fig7-quantitative-comparison.jpg" width="700px" class="img-responsive" alt="overview"></center>
				<center><image src="img/gray_line.jpg" class="img-responsive" width="700px" alt="overview"></center>
				<center><image src="img/fig8-data-scale-diversity.jpg" width="700px" class="img-responsive" alt="overview"></center>
				<center><image src="img/gray_line.jpg" class="img-responsive" width="700px" alt="overview"></center>
                <p class="text-justify">
					Above results have answered three questions raised earlier, demonstrating that BiDemoSyn can efficiently synthesize high-quality real-world demonstrations, enabling scalable and generalizable visuomotor policy training with minimal human input. <br>
                    <strong>A1: The efficiency and usability of BiDemoSyn have obvious advantages over baselines.</strong> <br>
					<strong>A2: Demonstrations obtained via BiDemoSyn can support scalable imitation learning.</strong> <br>
					<strong>A3: Policies trained on BiDemoSyn data can achieve generalization to unseen variations.</strong> </a>
                </p>

            </div>
        </div>

		
		
        <div class="row">
    		<div class="col-md-10 col-md-offset-1">	
				<h3>
                    ▶ ③ Visualization and Analysis of Real Robot Rollouts
                </h3>
				<center><image src="img/fig9-qualitative-results.jpg" width="900px" class="img-responsive" alt="overview"></center>
                <p class="text-justify">
					Above is the visualization of all six bimanual tasks performed on real robots. All models are trained and tested under the ID evaluations. EquiBot is chosen as the visuomotor policy. Key dual-arm coordination movements associated with each task are partially enlarged for quick review.</a>
                </p>
				<table style="width: 100%; border-collapse: collapse;">
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/real-rollout-t1-plugpen.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/real-rollout-t2-inserting.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/real-rollout-t3-unscrew.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/real-rollout-t4-pouring.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
					<tr>
						<td style="text-align: left;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/real-rollout-t5-pressing.mp4" type="video/mp4" />
						</video>
						</td>
						<td style="text-align: right;">
						<video id="v0" width="470px" autoplay loop muted>
						  <source src="videos/real-rollout-t6-reorient.mp4" type="video/mp4" />
						</video>
						</td>
					</tr>
				</table>
                <p class="text-justify">
					We here supplement more qualitative videos of real robot rollouts. The blue rectangle boxes in each video are the corresponding given one-shot demonstrations. The red rectangle boxes are results of novel configurations (e.g., new placements or instances). All related videos have been sped up 2x for faster viewing and checking. These results underscore BiDemoSyn's ability to handle real-world complexities (such as mechanical tolerances and imperfect perception), while also exposing limitations in dynamic force modulation (e.g., over-pressing bottles or lids).</a>
				</p>
				<image src="img/gray_line.jpg" class="img-responsive" alt="overview">
                <p class="text-justify">
					While policies trained with BiDemoSyn achieve high success rates, failure analysis reveals systematic challenges. Note that although the policy we trained is executed end-to-end (it establishes an implicit mapping from observations to action outputs), we can still align the analysis from the stage where its failure cases are located to find the core cause of the error. Finally, using the experimental results of EquiBot under out-of-distribution evaluations, we categorize failures into five types according to the task execution logic (mainly from the design module of BiDemoSyn):</a>
				</p>
				<center><image src="img/fig10-failure-cases-analysis.jpg" width="700px" class="img-responsive" alt="overview"></center>
                <p class="text-justify">
					As can be seen, the orientation estimation and initial grasp failures dominate, reflecting two core challenges: (1) current pose estimators struggle with symmetric or textureless objects (e.g., metal spoon or shovel), and (2) gripper-centric path planning lacks fine-grained contact modeling (e.g., avoiding pre-touch collisions for irregular shapes). Addressing these requires advances in category-agnostic pose estimation and short-horizon contact optimization, which are critical directions for our future work.</a>
				</p>
				
			</div>
        </div>
		
		
					
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{zhou2025one,
	title={One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation},
	author={Huayi Zhou, Kui Jia},
	journal={arXiv preprint arXiv:2511.xxxxx},
	year={2025}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We acknowledge the providers of various hardware used in this project, including the <a href="https://www.aubo-cobot.com/public/i5product3">Aubo-i5 robotic arm</a>, <a href="https://en.dh-robotics.com/product/pgi">DH gripper PGI-80-80</a>, and <a href="https://dexforce-3dvision.com/productinfo/1022811.html">kingfisher binocular camera</a>. 
                    <br><br>
                The website template was borrowed from <a href="https://jonbarron.info/">Jon Barron</a> and <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>.
                </p>
            </div>
        </div>
		
    </div>
</body>
</html>
