
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>DirectMHP</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://hnuzhy.github.io/projects/DirectMHP/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://hnuzhy.github.io/projects/DirectMHP/"/>
    <meta property="og:title" content="DirectMHP: Direct 2D Multi-Person Head Pose Estimation with Full-range Angles" />
    <meta property="og:description" content="Existing head pose estimation (HPE) mainly focuses on single person with pre-detected frontal heads, which limits their applications in real complex scenarios with multi-persons. We argue that these single HPE methods are fragile and inefficient for Multi-Person Head Pose Estimation (MPHPE) since they rely on the separately trained face detector that cannot generalize well to full viewpoints, especially for heads with invisible face areas. In this paper, we focus on the full-range MPHPE problem, and propose a direct end-to-end simple baseline named DirectMHP. Due to the lack of datasets applicable to the full-range MPHPE, we firstly construct two benchmarks by extracting ground-truth labels for head detection and head orientation from public datasets AGORA and CMU Panoptic. They are rather challenging for having many truncated, occluded, tiny and unevenly illuminated human heads. Then, we design a novel end-to-end trainable one-stage network architecture by joint regressing locations and orientations of multi-head to address the MPHPE problem. Specifically, we regard pose as an auxiliary attribute of the head, and append it after the traditional object prediction. Arbitrary pose representation such as Euler angles is acceptable by this flexible design. Then, we jointly optimize these two tasks by sharing features and utilizing appropriate multiple losses. In this way, our method can implicitly benefit from more surroundings to improve HPE accuracy while maintaining head detection performance. We present comprehensive comparisons with state-of-the-art single HPE methods on public benchmarks, as well as superior baseline results on our constructed MPHPE datasets. Datasets and code are released in https://github.com/hnuzhy/DirectMHP"/>



<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>âš¡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>DirectMHP</b>: Direct 2D Multi-Person Head Pose Estimation with Full-range Angles</br> 
                <small>
                Arxiv 2023
                </small></br>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://hnuzhy.github.io">Huayi Zhou</a>
                        </br>Shanghai Jiao Tong University (SJTU)
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=EiYbuesAAAAJ&hl=en">Fei Jiang</a>
                        </br>East China Normal University (ECNU)
                    </li>
                    <li>
                        <a href="https://www.cs.sjtu.edu.cn/PeopleDetail.aspx?id=95">Hongtao Lu</a>
                        </br>Shanghai Jiao Tong University (SJTU)
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2302.01110">
                            <image src="img/DirectMHP_image.jpg" height="60px">
                                <h4><strong>Arxiv</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/hnuzhy/DirectMHP">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <table style="width: 100%; border-collapse: collapse;">
          <tr>
            <td style="text-align: center;">
                <video id="v0" width="70%" autoplay loop muted>
                  <source src="img/000002_mpiinew_test_DirectMHP_vis3d.mp4" type="video/mp4" />
                </video>
            </td>
            <td style="text-align: center;">
                <video id="v0" width="70%" autoplay loop muted>
                  <source src="img/000003_mpiinew_test_DirectMHP_vis3d.mp4" type="video/mp4" />
                </video>
            </td>
          </tr>
          <tr>
            <td style="text-align: center;">000002_mpiinew_test</td>
            <td style="text-align: center;">000003_mpiinew_test</td>
          </tr>
        </table>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/illustration.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
Existing head pose estimation (HPE) mainly focuses on single person with pre-detected frontal heads, which limits their applications in real complex scenarios with multi-persons. We argue that these single HPE methods are fragile and inefficient for Multi-Person Head Pose Estimation (MPHPE) since they rely on the separately trained face detector that cannot generalize well to full viewpoints, especially for heads with invisible face areas. In this paper, we focus on the full-range MPHPE problem, and propose a direct end-to-end simple baseline named DirectMHP. Due to the lack of datasets applicable to the full-range MPHPE, we firstly construct two benchmarks by extracting ground-truth labels for head detection and head orientation from public datasets AGORA and CMU Panoptic. They are rather challenging for having many truncated, occluded, tiny and unevenly illuminated human heads. Then, we design a novel end-to-end trainable one-stage network architecture by joint regressing locations and orientations of multi-head to address the MPHPE problem. Specifically, we regard pose as an auxiliary attribute of the head, and append it after the traditional object prediction. Arbitrary pose representation such as Euler angles is acceptable by this flexible design. Then, we jointly optimize these two tasks by sharing features and utilizing appropriate multiple losses. In this way, our method can implicitly benefit from more surroundings to improve HPE accuracy while maintaining head detection performance. We present comprehensive comparisons with state-of-the-art single HPE methods on public benchmarks, as well as superior baseline results on our constructed MPHPE datasets.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Datasets: Single-Person HPE vs. Multi-Person HPE
                </h3>
                <p style="text-align:center;"><image src="img/full_range.png" width=90% alt="overview"></p><br>
                <p style="text-align:center;"><image src="img/datasetexamples.png" width=90% alt="overview"></p><br>
                <p class="text-justify">
                    Example images of two constructed challenging datasets: <strong>AGORA-HPE</strong> (top row) and <strong>CMU-HPE</strong> (middle row), and two widely used single HPE datasets: 300W-LP & AFLW2000 (left-down) and BIWI (right-down).
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    More Illustrations of CMU-HPE and AGORA-HPE 
                </h3>
                <p style="text-align:center;"><image src="img/snapshot.png" width=90% alt="overview"></p><br>
                <p class="text-justify">
                    Snapshot of 31 views from the sequence <strong>170307_dance5</strong> at sampling moment T = 24 seconds. Frames are cropped for ease of presentation.
                </p>
                <p style="text-align:center;"><image src="img/hardsamples.png" width=90% alt="overview"></p><br>
                <p class="text-justify">
                    Example of some challenging head samples from <strong>AGORA-HPE</strong> (first line) and <strong>CMU-HPE</strong> (second line). The third line is 3D head model indicator.
                </p>
                <p style="text-align:center;"><image src="img/statistic.png" width=90% alt="overview"></p><br>
                <p class="text-justify">
                    Pose label distribution of three Euler angles in datasets <strong>AGORA-HPE</strong>, <strong>CMU-HPE</strong>, 300W-LP & AFLW2000 and BIWI.
                </p>
            </div>
        </div>
           
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    More Visualization Results of DirectMHP
                </h3>
 
                <table style="width: 100%; border-collapse: collapse;">
                  <tr>
                    <td style="text-align: center;">
                        <image src="coco/000000002685.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                    <td style="text-align: center;">
                        <image src="coco/000000002685_vis3d_res.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                  </tr>
                </table>
                <table style="width: 100%; border-collapse: collapse;">
                  <tr>
                    <td style="text-align: center;">
                        <image src="coco/000000018380.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                    <td style="text-align: center;">
                        <image src="coco/000000018380_vis3d_res.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                  </tr>
                </table>
                <table style="width: 100%; border-collapse: collapse;">
                  <tr>
                    <td style="text-align: center;">
                        <image src="coco/000000038829.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                    <td style="text-align: center;">
                        <image src="coco/000000038829_vis3d_res.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                  </tr>
                </table>
                <table style="width: 100%; border-collapse: collapse;">
                  <tr>
                    <td style="text-align: center;">
                        <image src="coco/000000081988.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                    <td style="text-align: center;">
                        <image src="coco/000000081988_vis3d_res.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                  </tr>
                </table>
                <table style="width: 100%; border-collapse: collapse;">
                  <tr>
                    <td style="text-align: center;">
                        <image src="coco/000000160864.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                    <td style="text-align: center;">
                        <image src="coco/000000160864_vis3d_res.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                  </tr>
                </table>
                <table style="width: 100%; border-collapse: collapse;">
                   <td style="text-align: center;">
                        <image src="coco/000000183648.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                    <td style="text-align: center;">
                        <image src="coco/000000183648_vis3d_res.jpg" width=95% class="img-responsive" alt="overview">
                    </td>
                  </tr>
                </table>
 
                <p class="text-justify">
                    Visualization on some in-the-wild images from COCO val-set. As shown in Figures, in some challenging cases, especially the head sample with invisible face, our method generally does not have significant head orientation angle errors. This is largely supported by our end-to-end design which can exploit the relation of each head with its whole human body in the original images.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{zhou2023directmhp,
  title={DirectMHP: Direct 2D Multi-Person Head Pose Estimation with Full-range Angles},
  author={Zhou, Huayi and Jiang, Fei and Lu, Hongtao},
  journal={arXiv preprint arXiv:2302.01110},
  year={2023}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We acknowledge the effort from authors of project <a href="https://github.com/ultralytics/yolov5">YOLOv5</a> and both publish datasets <a href="https://agora.is.tue.mpg.de/index.html">AGORA</a> and <a href="http://domedb.perception.cs.cmu.edu/">CMU Panoptic Studio Dataset</a>. These datasets make our construction of AGORA-HPE and CMU-HPE possible. This paper was supported by NSFC (No. 62176155, 62207014), Shanghai Municipal Science and Technology Major Project, China, under grant no. 2021SHZDZX0102. 
                    <br><br>
                The website template was borrowed from <a href="https://jonbarron.info/">Jon Barron</a> and <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
